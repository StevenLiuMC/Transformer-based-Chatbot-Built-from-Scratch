# -*- coding: utf-8 -*-
"""Transformer based Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aKgtkJaSs9QMwpbNM21iHjBQ0ANKDNvy
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import re
from tqdm import tqdm
import math
import random

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============== Tokenizer ==============

class SimpleTokenizer:
  """
  A simple character or word-level tokenizer for the chatbot.
  This tokenizer builds a vocabulary from the training data and converts between text and tokens.
  For this simple tokenizer we only have two granularities - word or character. Subword level is not supported.
  """
  def __init__(self, level = "word"):
    """
    Args:
      level: "word" or "char" - determines tokenization granularity 决定分词粒度
    """
    self.level = level
    self.word2idx = {'<PAD>':0, '<UNK>':1, '<SOS>':2, '<EOS>':3} # This is a common practice in NLP tasks.
    self.idx2word = {0:'<PAD>', 1:'<UNK>', 2:'<SOS>', 3:'<EOS>'}
    self.vocab_size = 4 # Indicates the current vocabulary size, initially there are only 4 special tokens

  def build_vocab(self, texts, max_vocab_size = 10000): # The vocabulary contains at most 10,000 different words.
    """
    Build vocabulary from a list of texts.
    Args:
      texts: List of text strings to build vocabulary from.
      max_vocab_size: Maximum vocabulary size to keep.
    """
    word_freq = {}

    for text in texts:
      tokens = self._tokenize(text)
      for token in tokens:
        word_freq[token] = word_freq.get(token, 0) + 1 # dict.get(key, default=None)

    # Sort by frequency and keep top max_vocab_size
    sorted_words = sorted(word_freq.items(), key = lambda x: x[1], reverse = True)
    sorted_words = sorted_words[:max_vocab_size - 4] # Reserve 4 spots for special tokens

    # Add to vocabulary， insert them into the word2idx dictionary in descending order of word frequency.
    for word,_ in sorted_words:
      if word not in self.word2idx:
        self.word2idx[word] = self.vocab_size
        self.idx2word[self.vocab_size] = word
        self.vocab_size += 1
    print(f"Vocabulary size: {self.vocab_size}")

  def _tokenize(self, text):
    """
    Tokenize text based on level (in this case we use word-level).
    """
    text = text.lower().strip()
    if self.level == "word":
      # Simple word tokenization - you can improve this with better regex
      # Match one or more letters/numbers/underscores (words) or matches a non-alphanumeric and non-whitespace character (punctuation)
      tokens = re.findall(r'\w+|[^\w\s]',text) # \w+ 匹配一个或多个字母/数字/下划线（单词），| 表示"或"，[^\w\s] 匹配非字母数字且非空白的字符（标点符号）
    else: # Char level
      tokens = list(text)
    return tokens

  def encode(self, text, max_length = None):
    """
    Convert text to token indices.
    Args:
      text: Input text string to encode.
      max_length: Maximum sequence length (pad or truncate to this length).
    Returns:
      List of token indices.
    """
    tokens = self._tokenize(text)
    indices = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]

    if max_length:
      # Truncate and remain one spot for EOS
      if len(indices) >= max_length - 1:
        indices = indices[:max_length-1]

      # If sequence is not long enough, add EOS first to tell the model that the useful information has ended and then add PAD to make the indices dimension unified.
      indices.append(self.word2idx['<EOS>'])

      if len(indices) < max_length:
        indices += [self.word2idx['<PAD>']] * (max_length - len(indices))
      else:
      # If there's no limitation on length, simply add EOS
        indices.append(self.word2idx['<EOS>'])

    return indices

  def decode(self, indices):
    """
    Convert token indices back to text.
    Args:
      indices: List or tensor of token indices.
    Returns:
      Decoded text string.
    """
    if torch.is_tensor(indices):
      indices = indices.cpu().numpy()

    tokens = []
    for idx in indices:
      if idx == self.word2idx['<EOS>']:
        break
      if idx != self.word2idx['<PAD>'] and idx != self.word2idx['<SOS>']:
        tokens.append(self.idx2word.get(idx, '<UNK>'))

    if self.level == "word":
      return ' '.join(tokens)
    else:
      return ''.join(tokens)

# ============== Positional Encoding ==============

class PositionalEncoding(nn.Module):
  """
  Add positional encoding to embeddings to give the model information about the position of tokens in the sequence.
  Uses sine and cosine functions of different frequencies to encode the position.
  """
  def __init__(self, d_model, max_seq_length = 512, dropout_rate = 0.1): # 这里只是进行了至多512个位置的编码，并不代表送入的句子单词数量(seq_len)也是512
    super().__init__()
    # During training, randomly zeroes some of the elements of the input tensor with probability :attr:`p`
    self.dropout = nn.Dropout(dropout_rate)

    # Create positional encoding matrix
    pe = torch.zeros(max_seq_length, d_model)
    position = torch.arange(0, max_seq_length, dtype = float).unsqueeze(1)

    # Create divisor term for the sinusoidal pattern， equivalent to 1/10000^(2i/d_model)
    div_term = torch.exp(torch.arange(0, d_model, 2, dtype = float) * -(math.log(10000.0)/d_model)) # Compute the element-wise exponential of a tensor.

    # Apply sin to even indices
    pe[:, 0::2] = torch.sin(position * div_term) # start:stop:step, select all rows(which means all words), but only even columns 0, 2, 4...
    # Apply cos to odd indices
    pe[:, 1::2] = torch.cos(position * div_term) # Select all rows(which means all words), but only odd columns 1, 3, 5...

    """
    Add batch dimension and register as buffer (not a parameter), that means it will not be updated during training.
    self.register_buffer(name: str, tensor: torch.Tensor)
    After being saved in the buffer, this variable can be only acquired through the name (string) defined in the quotation marks.
    """
    pe = pe.unsqueeze(0) # Add a dimension to match the dimension corresponding to batch_size 补充一个维度来匹配batch_size对应的维度
    self.register_buffer('peMat', pe)

  def forward(self, x):
    """
    Args:
      x: Input tensor with shape (batch_size, seq_length, d_model)
      pe is created in __init__ and has a fixed size of (1, max_seq_length, d_model). For example, max_seq_length=512, precompute the encoding of all 512 positions
      According to the actual input sequence length requirement, the corresponding length is truncated from the pre-calculated 512 position codes
    """
    # Add positional encoding to input embeddings. 类内不同方法之间访问实例变量必须用self.
    x = x + self.peMat[:, :x.size(1), :]
    return self.dropout(x)

# ============== Multi-head Attention ==============

class MultiHeadAttention(nn.Module):
  """
  Multi-head attention mechanism - the core of Transformer architecture.
  Allows the model to focus on different parts of the input sequence simultaneously.
  """
  def __init__(self, d_model, num_heads, dropout_rate = 0.1):
    super().__init__()
    """
    Assertions are debugging tools used to check whether a condition is true while a program is running.
    If the condition is true, the program continues to execute;
    If the condition is false, the program stops executing and throws an AssertionError exception.
    """
    assert d_model % num_heads == 0 # d_model must be divisible by num_heads d_model一定要能被num_heads整除

    self.d_model = d_model
    self.num_heads = num_heads
    self.d_k = d_model // num_heads # d_k is the dimension of the keys and queries, also the dimension of each head

    # Linear projections for Q, K, V   Q、K、V的线性投影
    # Here we initialize the matrix to be square and later we'll create the multi-head rectangle matrix
    self.w_q = nn.Linear(d_model, d_model)
    self.w_k = nn.Linear(d_model, d_model)
    self.w_v = nn.Linear(d_model, d_model)

    """
    Output projection matrix. After the multi-head attention calculation is completed, the results of all heads are concatenated,
    and w_o projects the concatenated results back to the d_model dimension.
    """
    self.w_o = nn.Linear(d_model, d_model)
    self.dropout = nn.Dropout(dropout_rate)

  def forward(self, query, key, value, mask = None):
    """
    Args:
      query: Query tensor with shape (batch_size, seq_length_q, d_model)
      key: Key tensor with shape (batch_size, seq_length_k, d_model)
      value: Value tensor with shape (batch_size, seq_length_v, d_model
      mask: Optional mask tensor
      *** For self-attention (including encoder self-attention module and decoder masked self-attention module), the query, key, and value tensors are all the same.
          That means that their squences' lengths are the same -> seq_length_q = seq_length_k = seq_length_v
      *** For encoder-decoder attention (cross-attention module), the query tensor is the output of the decoder, and the key and value tensors are the output of the encoder.
          That means that their sequences' lengths are different -> seq_len_q = decoder序列长度 seq_len_k = seq_len_v = encoder序列长度
    Returns:
      Output tensor with shape (batch_size, seq_length, d_model)
      attention_weights: Attention weights for visualization
    """
    batch_size = query.size(0)
    seq_len_q = query.size(1)
    seq_len_k = key.size(1)
    seq_len_v = value.size(1)

    # 1.Project and split the d_model-dimensional vector into n_heads d_k-dimensional vectors
    Q = self.w_q(query).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1,2)
    K = self.w_k(key).view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1,2)
    V = self.w_v(value).view(batch_size, seq_len_v, self.num_heads, self.d_k).transpose(1,2)

    # 2.Calculate attention scores
    """
    Here Q's shape is (batch_size, num_heads, seq_len_q, d_k), while K's shape is also (batch_size, num_heads, seq_len_k, d_k).
    After transpose of K, K's shape is (batch_size, num_heads, d_k, seq_len_k).
    For this 4-dimension multiplication, we can simply regard it as doing the multiplication of (seq_len_q, d_k)*(d_k, seq_len_k) for batch_size * num_heads times simultaneously.
    After this multiplication, the shape of scores is (batch_size, num_heads, seq_len_q, seq_len_k).
    """
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)

    # 3.Apply mask if provided(for padding and future positions)
    if mask is not None:
      mask = mask.unsqueeze(1).unsqueeze(1) # Add head and query dimensions
      scores.masked_fill_(mask == 0, -1e9)
      """
      # If we have mask（True means we need to keep the original value）
      mask = torch.tensor([[True, True, False],
                  [True, False, False]])

      # Fill the position with the value we set (-1e9) where mask is False
      scores.masked_fill_(mask == False, -1e9)
      # Results: [[1.0, 2.0, -1e9],
      #       [4.0, -1e9, -1e9]]

      masked_fill_ 和 masked_fill 的主要区别在于前者是原地（in-place）操作，而后者是返回一个新张量（out-of-place）的操作。
      在 PyTorch 这类深度学习框架中，通常以后缀下划线 _ 来表示一个函数会直接修改调用它的张量（Tensor）本身，而不是返回一个修改后的副本。

      tensor.masked_fill_(mask, value)
        当你使用 tensor.masked_fill_(mask, value) 时，tensor 的内容会根据 mask 直接被修改。
        其中 mask 中为 True 的位置，tensor 中对应位置的元素会被替换为 value。这个操作不返回任何新的张量，而是直接改变 tensor。

      new_tensor = tensor.masked_fill(mask, value)
        当你使用 new_tensor = tensor.masked_fill(mask, value) 时，原始的 tensor 不会发生任何变化。
        这个操作会返回一个新的张量 new_tensor，这个新张量是 tensor 的一个副本，并在 mask 为 True 的位置填充了 value

      """

    # 4.Apply softmax
    attention_weights = F.softmax(scores, dim=-1) # attention weights shape is (batch_size, num_heads, seq_len_q, seq_len_k)
    attention_weights = self.dropout(attention_weights)

    # 5.Apply attention to values
    """
    No matter self-attention module or cross-attention module, seq_len_k = seq_len_v.
    attention_weights's shape: (batch_size, num_heads, seq_len_q, seq_len_k)
    V's shape: (batch_size, num_heads, seq_len_v, d_k)
    After multiplication, the shape of context is (batch_size, num_heads, seq_len_q, d_k)
    """
    context = torch.matmul(attention_weights, V)

    # 6.Concatenate heads and put through final linear layer.
    """
    (batch_size, num_heads, seq_len_q, d_k) -> (batch_size, seq_len_q, num_heads, d_k) -> (batch_size, seq_len_q, d_model)
    """
    context = context.transpose(1,2).contiguous().view(batch_size, seq_len_q, self.d_model)
    output = self.w_o(context) # Equal to context @ w_o + b. We need this simple FFN to make the previous multi-head matrices fully merge with each other.

    return output, attention_weights

# ============== Feed Forward Network ==============

class FeedForward(nn.Module):
  """
  Position-wise feed-forward network. After the multi-head attention calculation is completed,
    the results of all heads are concatenated and return output tensor with the shape of (batch_size, seq_len_q, d_model).
  This network will receive the output tensor and process each token (in this case word) independently.
    If the batch_size = 2, sequence_length = 3, we will use this FFN 2*3=6 times.
  In Attention is All You Need, d_model = 512, d_ff = 2048. But in this case we will define them later.

  Consists of two linear transformations with a ReLU activation in between.
  """
  def __init__(self, d_model, d_ff, dropout_rate = 0.1):
    super().__init__()
    self.linear1 = nn.Linear(d_model, d_ff)
    self.linear2 = nn.Linear(d_ff, d_model)
    self.dropout = nn.Dropout(dropout_rate)
    self.relu = nn.ReLU()

  def forward(self, x):
    """
    Args:
      x: Input tensor with shape (batch_size, seq_length, d_model)
    Returns:
      Output tensor with shape (batch_size, seq_length, d_model
    """
    x = self.linear1(x)
    x = self.relu(x)
    x = self.dropout(x)
    x = self.linear2(x)
    return x

# ============== Encoder Layer ==============

class EncoderLayer(nn.Module):
  """
  Single encoder layer consists of:
    1. Multi-head self-attention
    2. Position-wise feed-forward network
    Both with residual connections and layer normalization.
  """
  def __init__(self, d_model, num_heads, d_ff, dropout_rate = 0.1):
    super().__init__()
    # 这里创建了一个MultiHeadAttention的实例，并赋值给self.self_attention
    self.self_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)
    # 这里创建了一个FeedForward的实例，并赋值给self.feed_forward
    self.feed_forward = FeedForward(d_model, d_ff, dropout_rate)
    """
    虽然norm1和norm2看起来都是用相同的方式创建的nn.LayerNorm(d_model)，但它们实际上是两个完全独立的LayerNorm层，各自有自己的参数。
    在PyTorch中，每次调用nn.LayerNorm(d_model)都会创建一个新的LayerNorm实例，包含：
      1. 独立的scale参数（γ）：初始化为1
      2. 独立的shift参数（β）：初始化为0

      LayerNorm(x) = γ * (x - mean) / sqrt(variance + ε) + β

      norm1用于第一个残差连接（在self-attention之后）
      norm2用于第二个残差连接（在feed-forward之后）

    这两个normalization层会在训练过程中学习到不同的γ和β参数，以适应各自位置的特定需求。
    如果我们只用一个LayerNorm实例，那么这两个位置就会被迫共享相同的参数，这会限制模型的表达能力。
    """
    self.norm1 = nn.LayerNorm(d_model)
    self.norm2 = nn.LayerNorm(d_model)
    self.dropout = nn.Dropout(dropout_rate)

  def forward(self, x, mask = None):
    """
    Args:
      x: Input tensor with shape (batch_size, seq_length, d_model)
      mask: Optional mask tensor
    Returns:
      Output tensor with shape (batch_size, seq_length, d_model)
    """
    # Self-attention with residual connection and layer normalization.
    """
    这里有一个PyTorch的重要特性：当你对一个nn.Module的实例使用函数调用语法（加括号）时，PyTorch会自动调用它的forward方法。
    （但也只针对forward方法可以实现自动调用，其他方法还是需要显示调用的）
    attention_output has the shape of (batch_size, seq_len_q, d_model), while seq_len_q is sequence length of x.
    The result of attention_output is the result obtained after processing x by the forward method under MultiHeadAttention.这就是自注意力模块对输入张量 x 处理后的输出
    """
    attention_output, _ = self.self_attention(x, x, x, mask) # Equivalent to output = self.self_attention.forward(x, x, x, mask)
    x = self.norm1(x + self.dropout(attention_output))

    # Feed-forward with residual connection and layer normalization.
    ff_output = self.feed_forward(x)
    x = self.norm2(x + self.dropout(ff_output))

    return x

# ============== Decoder Layer ==============

class DecoderLayer(nn.Module):
  """
  Single decoder layer consists of:
    1. Masked multi-head self-attention
    2. Multi-head encoder-decoder cross-attention (attention to encoder output)
    3. Position-wise feed-forward network
    All with residual connections and layer normalization.
  """
  def __init__(self, d_model, num_heads, d_ff, dropout_rate = 0.1):
    super.__init__()
    self.masked_self_attention = MultiHeadAttention(d_model, num_heads, dropout_rate) # Create in-class
    self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)
    self.feed_forward = FeedForward(d_model, d_ff, dropout_rate)
    self.norm1 = nn.LayerNorm(d_model)
    self.norm2 = nn.LayerNorm(d_model)
    self.norm3 = nn.LayerNorm(d_model)
    self.dropout = nn.Dropout(dropout_rate)

  def forward(self, x, encoder_output, source_mask = None, target_mask = None):
    """
    Args:
      x: Input tensor with shape (batch_size, seq_length, d_model)
      encoder_output: Output tensor from encoder module with shape (batch_size, seq_length, d_model)
      source_mask: Source sequence mask tensor
      target_mask: Target sequence mask tensor (with future positions masked)
    Returns:
      Output tensor with shape (batch_size, seq_length, d_model)
    """
    # Masked self-attention
    self_attention_output, _ = self.masked_self_attention(x, x, x, target_mask)
    x = self.norm1(x + self.dropout(self_attention_output)) # Layer Normalization (Residual Connection + Self-attention Output(with softmax on d_model dimension))

    # Cross-attention with encoder output
    cross_attention_output, _ = self.cross_attention(x, encoder_output, encoder_output, source_mask) # Args: query, key, value, mask
    x = self.norm2(x + self.dropout(cross_attention_output))

    # Feed-forward
    ff_output = self.feed_forward(x)
    x = self.norm3(x + self.dropout(ff_output))

    return x

# ============== Transformer Model ==============

class TransformerChatbot(nn.Module):
  """
  Assembel all the needed modules to create a transformer chatbot.
  Consists of:
    1. Input embeddings and positional encoding
    2. Stack of encoder layers
    3. Stack of decoder layers
    4. Output projection to vocabulary
  """
  def __init__(self, vocab_size, d_model=256, num_heads=8, num_layers=4, d_ff=1024, max_seq_length=512, dropout_rate=0.1):
    super.__init__()

    # Model parameters
    self.d_model = d_model
    self.vocab_size = vocab_size
    self.max_seq_length = max_seq_length

    # Embeddings and positional encoding
    """
    By default, PyTorch uses the standard normal distribution N(0,1) to initialize the embedding matrix. This is equivalent to creating a lookup table of vocab_size × d_model.
      Before using nn.Embedding to transfer words in the vocabulary into embeddings, we need to transfer the words in the sequence into IDs.
      Then we proceed ID tensor with shape (batch_size, sequence_length) to nn.Embedding to transfer the ID into d_model-dimensional embeddings. This process is word embedding initialization.
      We use self.embedding like "self.embedding(src)", where src is the input tensor of word IDs with shape (batch_size, sequence_length).
    Although the initialization is random, during the training process,
      the word vectors are constantly adjusted through back propagation. Similar words will gradually move closer in high-dimensional space.
    After this transformation from ID to embeddings, we can have the word embedding matrix with shape (batch_size, sequence_length, d_model).
      This process can be concluded as a chain: Word -> ID -> Embedding. “词 → ID → 向量” 的映射链条就是词嵌入（Word Embedding）的本质
    """
    self.embedding = nn.Embedding(vocab_size, d_model)
    self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout_rate)

    # Encoder and Decoder stacks
    """
    nn.ModuleList is a special container in PyTorch that is specifically used to store neural network modules. It returns a ModuleList object and can be used like a normal list.
      ModuleList can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module methods.
      When you pass the tensor to the encoder layers, it does not automatically iterate over these 4 layers.
        You still need to manually create a corresponding forward function to complete the iteration of these 4 layers.
      Each nn.Module (such as EncoderLayer) put in will be automatically registered with the model,
        so that PyTorch will automatically track the parameters of these layers during training to ensure that they can be updated by the optimizer.
      If you use List [] to contain those layers, that's legal but all those parameters will not be registered and tracked, which also means that they will not be updated during training.
    """
    self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)])
    # for _ in range(n_layers)中的下划线_确实是一个占位符，表示"我需要循环num_layers次，但不需要使用循环变量"。这个列表推导式会创建num_layers个独立的EncoderLayer实例。
    self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout_rate) for _ in range(num_layers)])

    # Output projection
    self.output_projection = nn.Linear(d_model, vocab_size)

    # Initialize weights
    self._init_weights()

  def _init_weights(self):
    """
    Initialize model weights using Xavier initialization.
    Xavier初始化（也叫Glorot初始化）的数学原理是保持每层输入和输出的方差一致，防止梯度消失或爆炸。公式是从均匀分布U(-a, a)中采样，其中a = sqrt(6/(fan_in + fan_out))
    nn.init.kaiming_normal_：适合ReLU激活函数
    nn.init.normal_：标准正态分布
    nn.init.constant_：常数初始化
    nn.init.zeros_：全零初始化
    """
    for p in self.parameters(): # Returns an iterator over all trainable parameters in the model. This is typically passed to an optimizer.
      if p.dim() > 1:
        nn.init.xavier_uniform_(p)

  def create_padding_mask(self, seq, pad_idx=0):
    """
    Create a mask to hide padding tokens.
    Args:
      seq: Input sequence tensor with shape (batch_size, seq_length), contains words' ID.
      # seq may seems like this: （batch_size=2, seq_length=5）
      seq = torch.tensor([
          [15, 23, 8, 0, 0],  # "How are" + two paddings
          [42, 3, 0, 0, 0]    # "Hello" + three paddings
          ])
      pad_idx: Index of the padding token in the vocabulary (in this case the padding token is 0, check class SimpleTokenizer)
    Returns:
      Mask tensor where True values indicate valid positions.
      tensor([[True, True, True, False, False],
          [True, True, False, False, False]], dtype=torch.bool)
    """
    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)
    # -> (batch_size, 1, 1, seq_length), two dimensions with 1 so that it can be applied to attention score matrix with shape (batch_size, num_heads, seq_length, seq_length) through broadcasting

  def create_look_ahead_mask(self, size):
    """
    Create a mask to prevent attending to future positions. triu -> triangular upper 上三角矩阵
    关于torch.ones(size, size), 传入维度时不需要额外的括号或方括号，因为函数签名就是接受多个维度参数 torch.ones(2, 3, 4) -> 创建2×3×4张量;  torch.ones(3, 4) -> 创建3×4矩阵
    Args:
      size: Size of the square mask matrix
      diagonal: Diagonal offset (default 0 means main diagonal, 1 means above the main diagonal -> exclude the main diagonal)

      torch.triu(torch.ones(3,3), diagonal=0)
      # 保留主对角线及其上方
      => tensor([[1., 1., 1.],
            [0., 1., 1.],
            [0., 0., 1.]])

      torch.triu(torch.ones(3,3), diagonal=1)
      # 保留主对角线上方（不含主对角线）
      => tensor([[0., 1., 1.],
            [0., 0., 1.],
            [0., 0., 0.]])

    Returns:
      Upper triangular mask matrix, elements in the matrix that are equal to 0 become True
      tensor([[True, False, False],
          [True, True, False],
          [True, True, True]], dtype=torch.bool)
    """
    mask = torch.triu(torch.ones(size, size), diagonal=1).to(device)
    return mask == 0

  def forward(self, src, tgt):
    """
    Training Process
    Args:
      src: Source sequence tensor with shape (batch_size, src_seq_length)
      tgt: Target sequence tensor with shape (batch_size, tgt_seq_length)
      Both src and tgt are word ID tensors. Not original word tensors simply stripped from sequences. There is a transformation between words and word IDs.

    Returns:
      Output logits with shape (batch_size, tgt_seq_length, vocab_size)
    """
    # Create masks
    src_mask = self.create_padding_mask(src) # src's shape: (batch_size, seq_length). Tensor src is not a embedding tensor, it's just a word ID tensor.
    print(src_mask.shape())
    tgt_mask = self.create_padding_mask(tgt) & self.create_look_ahead_mask(tgt.size(1))
    print(tgt_mask.shape())

    # Embed and encode source sequence
    src_embedded = self.embedding(src) * math.sqrt(self.d_model) # Ensure that the gradient of the softmax function changes significantly.
    print(src_embedded.shape()) # Here we should get a tensor with shape: (batch_size, sequence_length, d_model)
    src_embedded = self.positional_encoding(src_embedded) # src_embedded will be proceed into Encoder Layers in the following step.

    enc_output = src_embedded
    for encoder_layer in self.encoder_layers:
      enc_output = encoder_layer(enc_output, src_mask)

    # Embed and decode target sequence
    tgt_embedded = self.embedding(tgt) * math.sqrt(self.d_model)
    print(tgt_embedded.shape())
    tgt_embedded = self.positional_encoding(tgt_embedded)

    dec_output = tgt_embedded
    for decoder_layer in self.decoder_layers:
      dec_output = decoder_layer(dec_output, enc_output, src_mask, tgt_mask)

    # Project to vocabulary
    output = self.output_projection(dec_output) # Logits tensor with shape (batch_size, tgt_seq_length, vocab_size)

    return output

  def generate(self, src, tokenizer, max_length=50, temperature=1.0):
    """
    Generate a response given a source sequence.

    Args:
      src: Source sequence tensor of shape (1, src_seq_length)
      tokenizer: Tokenizer object for decoding
      max_length: Maximum length of generated sequence
      temperature: Sampling temperature (higher = more random)

    Returns:
      Generated text string.
    """
    self.eval() # Set the model to evaluation mode. Turn off Dropout (no longer randomly drop neurons), turn off BatchNorm updates (if any), and notify all submodules to enter evaluation mode
    with torch.no_grad(): # torch.no_grad() 暂时关闭梯度计算
      # Encoder source
      src_mask = self.create_padding_mask(src)
      src_embedded = self.embedding(src) * math.sqrt(self.d_model)
      src_embedded = self.positional_encoding(src_embedded)
      enc_output = src_embedded
      for encoder_layer in self.encoder_layers: # Here we iterate four layers in that ModuleList object iterator
        enc_output = encoder_layer(enc_output, src_mask)

      # Start with SOS token, here the shape of tgt is (1,1) <- (batch_size=1, sequence_length=1)
      tgt = torch.tensor([[tokenizer.word2idx['<SOS>']]]).to(device)

      for _ in range(max_length):
        # Create target mask
        tgt_mask = self.create_look_ahead_mask(tgt.size(1))

        # Decoder
        tgt_embedded = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt_embedded = self.positional_encoding(tgt_embedded)
        dec_output = tgt_embedded # dec_output shape: (batch_size, sequence_length, d_model)
        for decoder_layer in self.decoder_layers:
          dec_output = decoder_layer(dec_output, enc_output, src_mask, tgt_mask) # return tensor shape: (batch_size, seq_length, d_model)

        # Get next token probabilities
        """
        在词汇表维度上进行softmax计算，使用温度来选择让分布区分度更明显还是更模糊
        T < 1：放大差异，使分布更尖锐（更确定） Amplify differences, make distributions sharper (more certain)
        T > 1：缩小差异，使分布更平坦（更随机） Reduce variance, make the distribution flatter (more random)
        """
        # Select last word's logits only, PyTorch will squeeze the middle dimension by default because we only select one index of that dimension, not a slice.
        logits = self.output_projection(dec_output[:, -1, :]) # dec_output[:, -1, :] shape: (batch_size, d_model); logits has shape: (batch_size, vocab_size). But in this module batch_size=1 so the shape is (1, vocab_size)
        # 每个样本一个分布，表示最后一个 token 预测的词概率。
        probs = F.softmax(logits / temperature, dim=-1) # probs has shape: (batch_size, vocab_size) -> (1, vocab_size).

        # Sample next token
        """
        torch.multinomial is a function in PyTorch used to sample integer indices from a given probability distribution.
          https://docs.pytorch.org/docs/stable/generated/torch.multinomial.html#torch-multinomial
        It is often used in language models to "extract words" according to probability distribution.
        torch.multinomial(input, num_samples, replacement=False)
          input	一维或二维 tensor，表示每个元素的“相对概率”
          num_samples	要采样的样本数量
          replacement	是否放回采样，默认 False（不重复）
          If input is a vector (1-dimension), output is a vector of size num_samples.
          If input is a matrix with m rows (2-dimension), output is an matrix of shape (m×num_samples).
        Return:
          Index of the selected elements.
        """
        # 注意：multinomial并不是一定选取概率最高的那个，而是采样num_samples次，至于每个元素被采集到的概率按照probs中的概率来计算。这样可以让模型的预测保留一定的creativity和更多的不确定性.
        next_token = torch.multinomial(probs, 1) # 这里采样获得的是词在vocabulary中对应的ID, shape: (batch_size, 1) -> (1, 1)
        # 把两个张量 tgt 和 next_token 沿着 dim=1(也就是sequence_length) 这个维度进行 拼接（concatenate），也就是在序列长度方向上追加一个新生成的token， dim=0是batch维度
        tgt = torch.cat([tgt, next_token], dim=1) # # (1, 1) + (1, 1) = (1, 2) 现在的 tgt = <SOS>的ID + 新词的ID

        # Stop if EOS token is generated
        # .item()方法将只包含一个元素的张量转换为Python的标准数据类型（int、float等），这里就是把这个张量tensor([[ID]])转换成标量ID
        if next_token.item() == tokenizer.word2idx['<EOS>']:
          break

      # Decode to text
      # tgt[0]提取第一个（也是唯一的）批次的数据, tgt的形状是[1, seq_length], tgt[0]的形状是[seq_length]
      # tokenizer.decode将token ID序列转换回文本字符串，这就是最终的回复
      return tokenizer.decode(tgt[0])
